{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.4.0"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2213138,"sourceType":"datasetVersion","datasetId":1329079}],"dockerImageVersionId":30749,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Survival Analysis with Machine Learning in Kaggle (COVID-19 Survival Dataset)\n\n## Default Kaggle Setup\n Purpose: Utilize the pre-installed tidyverse package and verify input files.\n Why: Kaggle's R environment includes tidyverse (dplyr, ggplot2, etc.), and input\n      data is available in ../input/. Listing files confirms the dataset's presence.\n```R","metadata":{}},{"cell_type":"code","source":"library(tidyverse)  # Pre-installed, includes dplyr, ggplot2, etc.\nlist.files(path = \"../input\")  # Should show \"covid19-survival-dataset\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:11:51.568471Z","iopub.execute_input":"2025-04-22T07:11:51.570408Z","iopub.status.idle":"2025-04-22T07:11:53.360249Z","shell.execute_reply":"2025-04-22T07:11:53.358614Z"}},"outputs":[{"name":"stderr","text":"── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.4     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.5\n\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.1\n\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.5.1     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.1\n\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"'covid19-survival-dataset'","text/markdown":"'covid19-survival-dataset'","text/latex":"'covid19-survival-dataset'","text/plain":"[1] \"covid19-survival-dataset\""},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## 1. Install and Load Additional Packages\n\nPurpose: Install and load packages for survival analysis, machine learning, and GPU support.\n\nWhy: We need survival for basic survival functions, randomForestSRC for Random\nSurvival Forests, gbm for Gradient Boosting, survex for explainable AI, and\nkeras for neural networks with GPU acceleration. Keras requires TensorFlow.","metadata":{}},{"cell_type":"code","source":"install.packages(c(\"randomForestSRC\", \"gbm\", \"xgboost\", \"survex\", \"keras\"))\nlibrary(survival)  # Survival functions (Surv object)\nlibrary(randomForestSRC)  # Random Survival Forests\nlibrary(gbm)  # Gradient Boosting\nlibrary(xgboost)  # Alternative boosting (not used but loaded)\nlibrary(survex)  # Explainable AI\nlibrary(keras)  # Neural networks\n# Configure keras with TensorFlow for GPU\ninstall_keras(method = \"conda\", conda = \"auto\")\n# Check GPU availability\nlibrary(reticulate)\ntf <- import(\"tensorflow\")\ncat(\"GPU Available:\", tf$test$is_gpu_available(), \"\\n\")\ncat(\"Packages loaded successfully!\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Load and Preprocess Data\n\nPurpose: Load SurvivalCovid.csv and prepare it for analysis.\n\nWhy: The dataset contains survival data that needs cleaning (e.g., handling missing\nvalues, converting status) and splitting into training/testing sets. Scaling\nnumerical features ensures compatibility with gradient boosting and neural\nnetworks, which perform better with standardized inputs.","metadata":{}},{"cell_type":"code","source":"data <- read_csv(\"../input/covid19-survival-dataset/SurvivalCovid.csv\")\n# Inspect data to confirm structure\nstr(data)\nsummary(data)\n# Remove rows with missing values\ndata <- data %>% drop_na()\n# Convert status: 1 (censored) -> 0, 2 (dead) -> 1\ndata <- data %>% mutate(status = status - 1)\n# Select relevant columns (exclude inst)\ndata <- data %>% select(time, status, age, sex, ph.ecog, ph.karno, pat.karno, meal.cal, wt.loss)\n# Split into training (80%) and testing (20%) sets\nset.seed(123)\ntrain_idx <- sample(1:nrow(data), 0.8 * nrow(data))\ntrain_data <- data[train_idx, ]\ntest_data <- data[-train_idx, ]\n# Scale numerical features\ntrain_data_scaled <- train_data\ntrain_data_scaled <- train_data_scaled %>% \n  mutate(\n    age = scale(age)[,1],\n    ph.karno = scale(ph.karno)[,1],\n    pat.karno = scale(pat.karno)[,1],\n    meal.cal = scale(meal.cal)[,1],\n    wt.loss = scale(wt.loss)[,1]\n  )\ntest_data_scaled <- test_data\ntest_data_scaled <- test_data_scaled %>% \n  mutate(\n    age = scale(age, \n                center = attr(train_data_scaled$age, \"scaled:center\"),\n                scale = attr(train_data_scaled$age, \"scaled:scale\"))[,1],\n    ph.karno = scale(ph.karno,\n                     center = attr(train_data_scaled$ph.karno, \"scaled:center\"),\n                     scale = attr(train_data_scaled$ph.karno, \"scaled:scale\"))[,1],\n    pat.karno = scale(pat.karno,\n                      center = attr(train_data_scaled$pat.karno, \"scaled:center\"),\n                      scale = attr(train_data_scaled$pat.karno, \"scaled:scale\"))[,1],\n    meal.cal = scale(meal.cal,\n                     center = attr(train_data_scaled$meal.cal, \"scaled:center\"),\n                     scale = attr(train_data_scaled$meal.cal, \"scaled:scale\"))[,1],\n    wt.loss = scale(wt.loss,\n                    center = attr(train_data_scaled$wt.loss, \"scaled:center\"),\n                    scale = attr(train_data_scaled$wt.loss, \"scaled:scale\"))[,1]\n  )\nsummary(train_data_scaled)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Random Survival Forests\n\nPurpose: Train a Random Survival Forest (RSF) model to predict survival outcomes.\n\nWhy: RSF handles censored data, modeling the cumulative hazard function. It captures\nnon-linear relationships and interactions, ideal for COVID-19 survival data.","metadata":{}},{"cell_type":"code","source":"rsf_model <- rfsrc(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno + meal.cal + wt.loss, \n                   data = train_data,\n                   ntree = 100,\n                   splitrule = \"logrank\",\n                   importance = TRUE)\nprint(rsf_model)\nrsf_pred <- predict(rsf_model, newdata = test_data)\nrsf_explainer <- explain(rsf_model, \n                        data = test_data %>% select(age, sex, ph.ecog, ph.karno, pat.karno, meal.cal, wt.loss), \n                        y = Surv(test_data$time, test_data$status))\nibs_rsf <- model_performance(rsf_explainer, type = \"ibs\")\nprint(ibs_rsf)\nplot(vimp(rsf_model))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Gradient Boosting\nPurpose: Train a Gradient Boosting Machine (GBM) model with Cox PH loss.\nWhy: GBM optimizes a Cox proportional hazards loss for censored data, often\nachieving high accuracy. Cross-validation tunes the number of trees.","metadata":{}},{"cell_type":"code","source":"gbm_model <- gbm(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno + meal.cal + wt.loss,\n                 data = train_data,\n                 distribution = \"coxph\",\n                 n.trees = 100,\n                 interaction.depth = 3,\n                 shrinkage = 0.01,\n                 cv.folds = 5)\nbest_iter <- gbm.perf(gbm_model, method = \"cv\", plot.it = TRUE)\ngbm_pred <- predict(gbm_model, newdata = test_data, n.trees = best_iter, type = \"link\")\ngbm_explainer <- explain(gbm_model,\n                        data = test_data %>% select(age, sex, ph.ecog, ph.karno, pat.karno, meal.cal, wt.loss),\n                        y = Surv(test_data$time, test_data$status),\n                        predict_function = function(model, newdata) \n                          predict(model, newdata, n.trees = best_iter, type = \"link\"))\nibs_gbm <- model_performance(gbm_explainer, type = \"ibs\")\nprint(ibs_gbm)\nsummary(gbm_model, n.trees = best_iter)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Neural Networks\n\nPurpose: Build a neural network with a custom Cox loss function for survival.\n\nWhy: Neural networks capture complex patterns, leveraging GPU for faster training.\nThe custom loss handles censoring. Included despite previous issues, with GPU check.","metadata":{}},{"cell_type":"code","source":"X_train <- as.matrix(train_data_scaled %>% select(age, sex, ph.ecog, ph.karno, pat.karno, meal.cal, wt.loss))\ny_train <- as.matrix(train_data_scaled %>% select(time, status))\nX_test <- as.matrix(test_data_scaled %>% select(age, sex, ph.ecog, ph.karno, pat.karno, meal.cal, wt.loss))\ny_test <- as.matrix(test_data_scaled %>% select(time, status))\ncox_loss <- function(y_true, y_pred) {\n  time <- y_true[, 1]\n  status <- y_true[, 2]\n  risk <- y_pred\n  log_risk <- k_log(risk)\n  uncensored_likelihood <- log_risk * status\n  cumsum_risk <- k_cumsum(k_exp(log_risk), axis = 1)\n  log_cumsum_risk <- k_log(cumsum_risk)\n  likelihood <- uncensored_likelihood - log_cumsum_risk\n  return(-k_mean(likelihood))\n}\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 16, activation = \"relu\", input_shape = ncol(X_train)) %>%\n  layer_dense(units = 8, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"linear\")\nmodel %>% compile(optimizer = \"adam\", loss = cox_loss)\nhistory <- model %>% fit(X_train, y_train, epochs = 50, batch_size = 32, \n                        validation_split = 0.2, verbose = 1)\nplot(history)\nnn_pred <- model %>% predict(X_test)\nnn_explainer <- explain(model,\n                        data = test_data_scaled %>% select(age, sex, ph.ecog, ph.karno, pat.karno, meal.cal, wt.loss),\n                        y = Surv(test_data$time, test_data$status),\n                        predict_function = function(model, newdata) \n                          model %>% predict(as.matrix(newdata)))\nibs_nn <- model_performance(nn_explainer, type = \"ibs\")\nprint(ibs_nn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Explainable AI\n\nPurpose: Interpret model predictions using SHAP and Partial Dependence Plots.\n\nWhy: Explainability is critical in biostatistics to understand which factors\n(e.g., age, ECOG score) drive survival predictions, aiding clinical decisions.","metadata":{}},{"cell_type":"code","source":"shap_rsf <- variable_importance(rsf_explainer, type = \"shap\")\nplot(shap_rsf)\nshap_gbm <- variable_importance(gbm_explainer, type = \"shap\")\nplot(shap_gbm)\npdp_rsf <- model_profile(rsf_explainer, variables = \"age\")\nplot(pdp_rsf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Compare Models\nPurpose: Summarize model performance to select the best approach.\nWhy: Comparing IBS scores identifies which model best predicts COVID-19 survival.","metadata":{}},{"cell_type":"code","source":"cat(\"Integrated Brier Scores:\\n\")\ncat(\"Random Survival Forest:\", ibs_rsf$result, \"\\n\")\ncat(\"Gradient Boosting:\", ibs_gbm$result, \"\\n\")\nif (exists(\"ibs_nn\")) cat(\"Neural Network:\", ibs_nn$result, \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Save Outputs\n\nPurpose: Save models and predictions for reproducibility and sharing.\n\nWhy: Kaggle’s /kaggle/working/ directory persists outputs for download or reuse.","metadata":{}},{"cell_type":"code","source":"saveRDS(rsf_model, \"/kaggle/working/rsf_model.rds\")\nsaveRDS(gbm_model, \"/kaggle/working/gbm_model.rds\")\nwrite_csv(as_tibble(rsf_pred$predicted), \"/kaggle/working/rsf_predictions.csv\")\nwrite_csv(as_tibble(gbm_pred), \"/kaggle/working/gbm_predictions.csv\")\nif (exists(\"nn_pred\")) write_csv(as_tibble(nn_pred), \"/kaggle/working/nn_predictions.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}